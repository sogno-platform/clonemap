# Copyright 2020 Institute for Automation of Complex Power Systems,
# E.ON Energy Research Center, RWTH Aachen University
#
# This project is licensed under either of
# - Apache License, Version 2.0
# - MIT License
# at your option.
#
# Apache License, Version 2.0:
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# MIT License:
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

# ------------------- Namespace ------------------- #

apiVersion: v1
kind: Namespace
metadata:
  name: clonemap
---

# ------------------- etcd client service ------------------- #

apiVersion: v1
kind: Service
metadata:
  name: etcd-cluster-client
  labels:
    etcd_cluster: etcd-cl
  namespace: clonemap
spec:
  ports:
  - name: etcd-client
    port: 2379
  selector:
    etcd_cluster: etcd-cl
---

# ------------------- etcd cluster service ------------------- #

apiVersion: v1
kind: Service
metadata:
  annotations:
  name: etcd-cl
  labels:
    etcd_cluster: etcd-cl
  namespace: clonemap
spec:
  clusterIP: None
  ports:
  - port: 2379
    name: client
  - port: 2380
    name: peer
  publishNotReadyAddresses: true
  selector:
    etcd_cluster: etcd-cl
---

# ------------------- etcd budget ------------------- #

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: etcd-cl-pdb
  namespace: clonemap
spec:
  minAvailable: 51%
  selector:
    matchLabels:
      etcd_cluster: etcd-cl
---

# ------------------- etcd stateful set ------------------- #

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: etcd-cl
  labels:
    etcd_cluster: etcd-cl
  namespace: clonemap
spec:
  serviceName: etcd-cl
  selector:
    matchLabels:
      etcd_cluster: etcd-cl
  replicas: 3
  podManagementPolicy: Parallel
  template:
    metadata:
      name: etcd-cl
      labels:
        app: etcd
        etcd_cluster: etcd-cl
        plane: control
      namespace: clonemap
    spec:
      containers:
      - name: etcd
        image: quay.io/coreos/etcd:v3.4.3
        ports:
        - containerPort: 2379
          name: client
        - containerPort: 2380
          name: server
        # yamllint disable rule:indentation rule:line-length
        command:
        - /bin/sh
        - -ec
        - |
          HOSTNAME=$(hostname)
          eps() {
            EPS=""
            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
              EPS="${EPS}${EPS:+,}http://${CLUSTER_NAME}-${i}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2379"
            done
            echo ${EPS}
          }
          member_hash() {
            etcdctl \
                --endpoints=$(eps) \
                member list | grep http://${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2380 | cut -d',' -f1
          }
          num_existing() {
            etcdctl \
                --endpoints=$(eps) \
                member list | wc -l
          }
          initial_peers() {
            PEERS=""
            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
              PEERS="${PEERS}${PEERS:+,}${CLUSTER_NAME}-${i}=http://${CLUSTER_NAME}-${i}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2380"
            done
            echo ${PEERS}
          }
          MEMBER_HASH=$(member_hash)
          EXISTING=$(num_existing)
          # Re-joining after failure?
          if [ -n "${MEMBER_HASH}" ]; then
            echo "Re-joining member ${HOSTNAME}"
            etcdctl \
                --endpoints=$(eps) \
                member remove ${MEMBER_HASH}
            rm -rf /var/run/etcd/*
            mkdir -p /var/run/etcd/
          fi
          if [ ${EXISTING} -gt 0 ]; then
            while true; do
              echo "Waiting for ${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE} to come up"
              ping -W 1 -c 1 ${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE} > /dev/null && break
              sleep 1s
            done
            etcdctl \
                --endpoints=$(eps) \
                member add ${HOSTNAME} --peer-urls=http://${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs
            if [ $? -ne 0 ]; then
              echo "Member add ${HOSTNAME} error"
              rm -f /var/run/etcd/new_member_envs
              exit 1
            fi
            cat /var/run/etcd/new_member_envs
            . /var/run/etcd/new_member_envs
            exec etcd --name ${HOSTNAME} \
                --initial-advertise-peer-urls http://${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2380 \
                --listen-peer-urls http://0.0.0.0:2380 \
                --listen-client-urls http://0.0.0.0:2379 \
                --advertise-client-urls http://${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2379 \
                --data-dir /var/run/etcd/default.etcd \
                --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE} \
                --max-request-bytes 2000000 \
                --max-wals 1 \
                --max-snapshots 1 \
                --quota-backend-bytes 8589934592 \
                --snapshot-count 5000
          fi
          for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
            while true; do
              echo "Waiting for ${CLUSTER_NAME}-${i}.${CLUSTER_NAME}.${POD_NAMESPACE} to come up"
              ping -W 1 -c 1 ${CLUSTER_NAME}-${i}.${CLUSTER_NAME}.${POD_NAMESPACE} > /dev/null && break
              sleep 1s
            done
          done
          echo "Joining member ${HOSTNAME}"
          exec etcd --name ${HOSTNAME} \
              --initial-advertise-peer-urls http://${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2380 \
              --listen-peer-urls http://0.0.0.0:2380 \
              --listen-client-urls http://0.0.0.0:2379 \
              --advertise-client-urls http://${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2379 \
              --initial-cluster-token etcd-cluster-1 \
              --data-dir /var/run/etcd/default.etcd \
              --initial-cluster $(initial_peers) \
              --initial-cluster-state new \
              --max-request-bytes 2000000 \
              --max-wals 1 \
              --max-snapshots 1 \
              --quota-backend-bytes 8589934592 \
              --snapshot-count 5000
        # yamllint enable rule:indentation rule:line-length
        env:
        - name: INITIAL_CLUSTER_SIZE
          value: '3'
        - name: CLUSTER_NAME
          # This has to match the metadata.name for things to work.
          value: etcd-cl
        - name: ETCDCTL_API
          value: '3'
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: ETCD_AUTO_COMPACTION_RETENTION
          value: '5'
        - name: ETCD_AUTO_COMPACTION_MODE
          value: revision
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -ec
            - etcdctl --endpoints=http://localhost:2379
              endpoint status
          failureThreshold: 3
          initialDelaySeconds: 1
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 5
        lifecycle:
          preStop:
            exec:
              # yamllint disable rule:indentation
              command:
              - /bin/sh
              - -ec
              - |
                HOSTNAME=$(hostname)
                member_hash() {
                  etcdctl \
                      member list | grep http://${HOSTNAME}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2380 | cut -d',' -f1
                }
                eps() {
                  EPS=""
                  for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                    EPS="${EPS}${EPS:+,}http://${CLUSTER_NAME}-${i}.${CLUSTER_NAME}.${POD_NAMESPACE}.svc:2379"
                  done
                  echo ${EPS}
                }
                MEMBER_HASH=$(member_hash)
                # Removing member from cluster
                if [ -n "${MEMBER_HASH}" ]; then
                  echo "Removing ${HOSTNAME} from etcd cluster"
                  etcdctl \
                      --endpoints=$(eps) \
                      member remove $(member_hash)
                  if [ $? -eq 0 ]; then
                    # Remove everything otherwise the cluster will no longer scale-up
                    rm -rf /var/run/etcd/*
                  fi
                fi
              # yamllint enable rule:indentation

---

# ------------------- mqtt cluster Service ------------------- #

apiVersion: v1
kind: Service
metadata:
  name: mqtt
  namespace: clonemap
  labels:
    app: mosquitto
spec:
  ports:
    - port: 1883
      name: mqtt
  clusterIP: None
  selector:
    app: mosquitto
---

# ------------------- mqtt external Service ------------------- #

apiVersion: v1
kind: Service
metadata:
  name: mqtt-np
  namespace: clonemap
  labels:
    app: mosquitto
spec:
  type: NodePort
  ports:
    - port: 1883
      protocol: TCP
      nodePort: 30883
      name: mqtt
  selector:
    app: mosquitto
---

# ------------------- mqtt ConfigMap ------------------- #

apiVersion: v1
kind: ConfigMap
metadata:
  name: mosquitto-config
  namespace: clonemap
data:
  conf:
    log_type none
    log_dest none
    set_tcp_nodelay true
---

# ------------------- mqtt Deployment ------------------- #

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mosquitto
  namespace: clonemap
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mosquitto
  template:
    metadata:
      labels:
        app: mosquitto
    spec:
      containers:
      - name: mosquitto
        image: eclipse-mosquitto:1.6.13
        ports:
          - containerPort: 1883
        readinessProbe:
          tcpSocket:
            port: 1883
          initialDelaySeconds: 5
          periodSeconds: 10
        volumeMounts:
        - name: config-volume
          mountPath: /mosquitto/config
      # nodeSelector:
      #   host: pan4
      volumes:
      - name: config-volume
        configMap:
          name: mosquitto-config
          items:
          - key: conf
            path: mosquitto.conf
---

# ------------------- ams Service Account ------------------- #

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: ams
  name: ams
  namespace: clonemap
---

# ------------------- ams Role ------------------- #

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ams-role
  namespace: clonemap
rules:
- apiGroups: ["", "apps"]
  resources: ["deployments", "pods", "services", "statefulsets"]
  verbs: ["create", "get", "update", "delete", "list"]
---

# ------------------- ams RoleBinding ------------------- #

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ams
  namespace: clonemap
  labels:
    k8s-app: ams
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ams-role
subjects:
- kind: ServiceAccount
  name: ams
---

# ------------------- ams Service ------------------- #

apiVersion: v1
kind: Service
metadata:
  namespace: clonemap
  name: ams
  labels:
    app: ams
spec:
  type: NodePort
  ports:
  - port: 9000
    protocol: TCP
    targetPort: ams-port
    nodePort: 30009
  selector:
    app: ams
    role: frontend
---

# ------------------- ams Deployment ------------------- #

apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: clonemap
  name: ams-deployment
  labels:
    app: ams
spec:
  selector:
    matchLabels:
      app: ams
  template:
    metadata:
      namespace: clonemap
      labels:
        app: ams
        role: frontend
    spec:
      containers:
      - name: ams-container
        image: clonemap/ams
        env:
          - name: CLONEMAP_DEPLOYMENT_TYPE
            value: "production"
          - name: CLONEMAP_RESOURCE_LIMITATION
            value: "NO"
          - name: CLONEMAP_STORAGE_TYPE
            value: "etcd"
          - name: CLONEMAP_LOG_LEVEL
            value: "error"
          - name: CLONEMAP_NAMESPACE
            value: "clonemap"
        # resources:
        #   requests:
        #     memory: "128Mi"
        #     cpu: "500m"
        #   limits:
        #     memory: "256Mi"
        #     cpu: "600m"
        ports:
        - containerPort: 9000
          name: ams-port
        livenessProbe:
          httpGet:
            path: /api/clonemap
            port: 9000
          initialDelaySeconds: 30
          timeoutSeconds: 20
      serviceAccountName: ams
---

# ------------------- Cassandra configmap ------------------- #

apiVersion: v1
data:
  cassandra-init.sh: |
    cat >/import.cql <<EOF
    CREATE KEYSPACE IF NOT EXISTS clonemap WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;
    CREATE TABLE clonemap.logging_error ( masid int, agentid int, t timestamp, log varchar, PRIMARY KEY ((masid, agentid), t)) WITH CLUSTERING ORDER BY (t ASC);
    CREATE TABLE clonemap.logging_app ( masid int, agentid int, t timestamp, log varchar, PRIMARY KEY ((masid, agentid), t)) WITH CLUSTERING ORDER BY (t ASC);
    CREATE TABLE clonemap.logging_msg ( masid int, agentid int, t timestamp, log varchar, PRIMARY KEY ((masid, agentid), t)) WITH CLUSTERING ORDER BY (t ASC);
    CREATE TABLE clonemap.logging_status ( masid int, agentid int, t timestamp, log varchar, PRIMARY KEY ((masid, agentid), t)) WITH CLUSTERING ORDER BY (t ASC);
    CREATE TABLE clonemap.logging_debug ( masid int, agentid int, t timestamp, log varchar, PRIMARY KEY ((masid, agentid), t)) WITH CLUSTERING ORDER BY (t ASC);
    CREATE TABLE clonemap.logging_beh ( masid int, agentid int, t timestamp, log varchar, PRIMARY KEY ((masid, agentid), t)) WITH CLUSTERING ORDER BY (t ASC);
    CREATE TABLE clonemap.logging_series ( masid int, agentid int, name varchar, t timestamp, series varchar, PRIMARY KEY ((masid, agentid), name, t)) WITH CLUSTERING ORDER BY (name ASC, t ASC);
    CREATE TABLE clonemap.heatmap ( masid int, t timestamp, item varchar, PRIMARY KEY (masid, t)) WITH CLUSTERING ORDER BY (t ASC);
    CREATE TABLE clonemap.beh_stats ( masid int, agentid int, behType varchar, start timestamp, stats varchar, PRIMARY KEY ((masid, agentid, behType), start)) WITH CLUSTERING ORDER BY (start ASC);
    CREATE TABLE clonemap.state ( masid int, agentid int, state varchar, PRIMARY KEY (masid, agentid));
    EOF

    until cqlsh cassandra -f /import.cql; do
    echo "cqlsh: Cassandra is unavailable to initialize - will retry later"
    sleep 2
    done
kind: ConfigMap
metadata:
  name: cassinit
  namespace: clonemap
---

# ------------------- Cassandra service ------------------- #

apiVersion: v1
kind: Service
metadata:
  namespace: clonemap
  name: cassandra
  labels:
    app: cass
spec:
  ports:
  - port: 9042
    protocol: TCP
    targetPort: cql-port
  selector:
    app: cass
---

# ------------------- Cassandra deployment ------------------- #

apiVersion: apps/v1
kind: StatefulSet
metadata:
  namespace: clonemap
  name: cass-ssset
  labels:
    app: cass
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cass
  template:
    metadata:
      labels:
        app: cass
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cass-container
        image: cassandra:3
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql-port
        #resources:
        #  limits:
        #    cpu: "2"
        #    memory: "4Gi"
        #  requests:
        #    cpu: "1"
        #    memory: "2Gi"
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command: 
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cass-ssset-0.cassandra.clonemap.svc"
          #- name: CASSANDRA_CLUSTER_NAME
          #  value: "K8Demo"
          #- name: CASSANDRA_DC
          #  value: "DC1-K8Demo"
          #- name: CASSANDRA_RACK
          #  value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
---

# ------------------- Cassandra init job ------------------- #

apiVersion: batch/v1
kind: Job
metadata:
  name: cassinit-job
  namespace: clonemap
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: cassinit-container
        image: cassandra
        volumeMounts:
        - name: init
          mountPath: /cassinit
        command: ["sh", "cassinit/cassandra-init.sh"]
      restartPolicy: Never
      volumes:
      - name: init
        configMap:
          name: cassinit
---

# ------------------- logger Service ------------------- #

apiVersion: v1
kind: Service
metadata:
  namespace: clonemap
  name: logger
  labels:
    app: logger
spec:
  type: NodePort
  ports:
  - port: 11000
    protocol: TCP
    targetPort: logger-port
    nodePort: 30011
  selector:
    app: logger
    role: frontend
---

# ------------------- logger Deployment ------------------- #

apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: clonemap
  name: logger-deployment
  labels:
    app: logger
spec:
  selector:
    matchLabels:
      app: logger
  template:
    metadata:
      namespace: clonemap
      labels:
        app: logger
        role: frontend
    spec:
      containers:
      - name: logger-container
        image: clonemap/logger
        env:
          - name: CLONEMAP_DEPLOYMENT_TYPE
            value: "production"
          - name: CLONEMAP_LOG_LEVEL
            value: "info"
        # resources:
        #   requests:
        #     memory: "128Mi"
        #     cpu: "300m"
        #   limits:
        #     memory: "256Mi"
        #     cpu: "400m"
        ports:
        - containerPort: 11000
          name: logger-port
        livenessProbe:
          httpGet:
            path: /api/logging/0/0/error/latest/1
            port: 11000
          initialDelaySeconds: 30
          timeoutSeconds: 20
---

# ------------------- DF Service ------------------- #

apiVersion: v1
kind: Service
metadata:
  namespace: clonemap
  name: df
  labels:
    app: df
spec:
  type: NodePort
  ports:
  - port: 12000
    protocol: TCP
    targetPort: df-port
    nodePort: 30012
  selector:
    app: df
    role: frontend
---

# ------------------- df Deployment ------------------- #

apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: clonemap
  name: df-deployment
  labels:
    app: df
spec:
  selector:
    matchLabels:
      app: df
  template:
    metadata:
      namespace: clonemap
      labels:
        app: df
        role: frontend
    spec:
      containers:
      - name: df-container
        image: clonemap/df
        env:
          - name: CLONEMAP_DEPLOYMENT_TYPE
            value: "production"
          - name: CLONEMAP_LOG_LEVEL
            value: "error"
        # resources:
        #   requests:
        #     memory: "128Mi"
        #     cpu: "300m"
        #   limits:
        #     memory: "256Mi"
        #     cpu: "400m"
        ports:
        - containerPort: 12000
          name: df-port
        livenessProbe:
          httpGet:
            path: /api/df/0/svc
            port: 12000
          initialDelaySeconds: 30
          timeoutSeconds: 20
---

# ------------------- Frontend Service ------------------- #

apiVersion: v1
kind: Service
metadata:
  namespace: clonemap
  name: fe
  labels:
    app: fe
spec:
  type: LoadBalancer
  ports:
  - port: 13000
    protocol: TCP
    targetPort: fe-port
    nodePort: 30013
  selector:
    app: fe
    role: frontend
---

# -------------------- Frontend Deployment -------------------- #

apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: clonemap
  name: fe-deployment
  labels:
    app: fe
spec:
  selector:
    matchLabels:
      app: fe
  template:
    metadata:
      namespace: clonemap
      labels:
        app: fe
        role: frontend
    spec:
      containers:
      - name: fe-container
        image: clonemap/frontend
        env:
          - name: CLONEMAP_DEPLOYMENT_TYPE
            value: "production"
          - name: CLONEMAP_LOG_LEVEL
            value: "error"
        # resources:
        #   requests:
        #     memory: "256Mi"
        #     cpu: "500m"
        #   limits:
        #     memory: "512Mi"
        #     cpu: "1"
        ports:
        - containerPort: 13000
          name: fe-port
        livenessProbe:
          httpGet:
            path: /api/overview
            port: 13000
          initialDelaySeconds: 30
          timeoutSeconds: 20